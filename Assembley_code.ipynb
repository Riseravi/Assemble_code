{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c70921",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3822dc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Ravi kumar'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97c64276",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = r\"C:\\Users\\Ravi kumar\\Downloads\\riscv_arithmetic_basic_test_4.S\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c709ee48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 486992 characters\n"
     ]
    }
   ],
   "source": [
    "# Read, then decode for py2 compat.\n",
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "# length of text is the number of characters in it\n",
    "print(f'Length of text: {len(text)} characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21c6e003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".include \"user_define.h\"\n",
      ".globl _start\n",
      ".section .text\n",
      "_start:           \n",
      "                  .include \"user_init.s\"\n",
      "                  csrr x5, 0xf14\n",
      "                  li x6, 0\n",
      "                  beq x5, x6, 0f\n",
      "\n",
      "0: la x29, h0_start\n",
      "jalr x0, x29, 0\n",
      "h0_sta\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the first 250 characters in text\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bea6f0df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afeef186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67 unique characters\n"
     ]
    }
   ],
   "source": [
    "# The unique characters in the file\n",
    "vocab = sorted(set(text))\n",
    "print(f'{len(vocab)} unique characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4ca13aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_texts = ['abcdefg', 'xyz']\n",
    "\n",
    "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
    "chars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292770b4",
   "metadata": {},
   "source": [
    "Process the text\n",
    "Vectorize the text\n",
    "Before training, you need to convert the strings to a numerical representation.\n",
    "\n",
    "The tf.keras.layers.StringLookup layer can convert each character into a numeric ID. It just needs the text to be split into tokens first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8470886d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_from_chars = tf.keras.layers.StringLookup(\n",
    "    vocabulary=list(vocab), mask_token=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0dad957",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[42, 43, 44, 45, 46, 47, 48], [65, 66, 67]]>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = ids_from_chars(chars)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56d7d7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars_from_ids = tf.keras.layers.StringLookup(\n",
    "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33319bc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = chars_from_ids(ids)\n",
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31d6575d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'abcdefg', b'xyz'], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.strings.reduce_join(chars, axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dbbecc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_from_ids(ids):\n",
    "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "805ea4ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(486992,), dtype=int64, numpy=array([ 9, 50, 55, ..., 65, 10,  1], dtype=int64)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
    "all_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0198dcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69b0dae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "i\n",
      "n\n",
      "c\n",
      "l\n",
      "u\n",
      "d\n",
      "e\n",
      " \n",
      "\"\n"
     ]
    }
   ],
   "source": [
    "for ids in ids_dataset.take(10):\n",
    "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "afdd62d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'.' b'i' b'n' b'c' b'l' b'u' b'd' b'e' b' ' b'\"' b'u' b's' b'e' b'r'\n",
      " b'_' b'd' b'e' b'f' b'i' b'n' b'e' b'.' b'h' b'\"' b'\\n' b'.' b'g' b'l'\n",
      " b'o' b'b' b'l' b' ' b'_' b's' b't' b'a' b'r' b't' b'\\n' b'.' b's' b'e'\n",
      " b'c' b't' b'i' b'o' b'n' b' ' b'.' b't' b'e' b'x' b't' b'\\n' b'_' b's'\n",
      " b't' b'a' b'r' b't' b':' b' ' b' ' b' ' b' ' b' ' b' ' b' ' b' ' b' '\n",
      " b' ' b' ' b'\\n' b' ' b' ' b' ' b' ' b' ' b' ' b' ' b' ' b' ' b' ' b' '\n",
      " b' ' b' ' b' ' b' ' b' ' b' ' b' ' b'.' b'i' b'n' b'c' b'l' b'u' b'd'\n",
      " b'e' b' ' b'\"' b'u' b's' b'e' b'r' b'_' b'i' b'n' b'i' b't' b'.' b's'\n",
      " b'\"' b'\\n' b' ' b' ' b' ' b' ' b' ' b' ' b' ' b' ' b' ' b' ' b' ' b' '\n",
      " b' ' b' ' b' ' b' ' b' ' b' ' b'c' b's' b'r' b'r' b' ' b'x' b'5' b','\n",
      " b' ' b'0' b'x' b'f' b'1' b'4' b'\\n' b' ' b' ' b' ' b' ' b' ' b' ' b' '\n",
      " b' ' b' ' b' ' b' ' b' ' b' ' b' ' b' ' b' ' b' ' b' ' b'l' b'i' b' '\n",
      " b'x' b'6' b',' b' ' b'0' b'\\n' b' ' b' ' b' ' b' ' b' ' b' ' b' ' b' '\n",
      " b' ' b' ' b' ' b' ' b' ' b' ' b' ' b' ' b' ' b' ' b'b' b'e' b'q' b' '\n",
      " b'x' b'5' b',' b' ' b'x' b'6' b',' b' ' b'0' b'f' b'\\n' b'\\n' b'0' b':'\n",
      " b' ' b'l' b'a' b' ' b'x' b'2' b'9' b',' b' ' b'h' b'0' b'_' b's' b't'\n",
      " b'a' b'r' b't' b'\\n' b'j' b'a' b'l' b'r' b' ' b'x' b'0' b',' b' ' b'x'\n",
      " b'2' b'9' b',' b' ' b'0' b'\\n' b'h' b'0' b'_' b's' b't' b'a' b'r' b't'\n",
      " b':' b'\\n' b' ' b' ' b' ' b' ' b' ' b' ' b' ' b' ' b' ' b' ' b' ' b' '\n",
      " b' ' b' ' b' ' b' ' b' ' b' ' b'l' b'i' b' ' b'x' b'1' b'8' b',' b' '\n",
      " b'0' b'x' b'4' b'0' b'0' b'0' b'1' b'1' b'0' b'4' b'\\n' b' ' b' ' b' '\n",
      " b' ' b' ' b' ' b' ' b' ' b' ' b' ' b' ' b' ' b' ' b' ' b' ' b' ' b' '\n",
      " b' ' b'c' b's' b'r' b'w' b' ' b'0' b'x' b'3' b'0' b'1' b',' b' ' b'x'\n",
      " b'1' b'8' b'\\n' b'k' b'e' b'r' b'n' b'e' b'l' b'_' b's' b'p' b':' b' '\n",
      " b' ' b' ' b' ' b' ' b' ' b' ' b' ' b'\\n' b' ' b' ' b' ' b' ' b' ' b' '\n",
      " b' ' b' ' b' ' b' ' b' ' b' ' b' ' b' ' b' ' b' ' b' ' b' ' b'l' b'a'\n",
      " b' ' b'x' b'1' b'3' b',' b' ' b'k' b'e' b'r' b'n' b'e' b'l' b'_' b's'\n",
      " b't' b'a' b'c' b'k' b'_' b'e' b'n' b'd' b'\\n' b'\\n' b't' b'r' b'a' b'p'\n",
      " b'_' b'v' b'e' b'c' b'_' b'i' b'n' b'i' b't'], shape=(401,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "seq_length = 400\n",
    "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for seq in sequences.take(1):\n",
    "    print(chars_from_ids(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff1ca97b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'.include \"user_define.h\"\\n.globl _start\\n.section .text\\n_start:           \\n                  .include \"user_init.s\"\\n                  csrr x5, 0xf14\\n                  li x6, 0\\n                  beq x5, x6, 0f\\n\\n0: la x29, h0_start\\njalr x0, x29, 0\\nh0_start:\\n                  li x18, 0x40001104\\n                  csrw 0x301, x18\\nkernel_sp:        \\n                  la x13, kernel_stack_end\\n\\ntrap_vec_init'\n",
      "b':    \\n                  la x18, mtvec_handler\\n                  ori x18, x18, 0\\n                  csrw 0x305, x18 # MTVEC\\n\\nmepc_setup:       \\n                  la x18, init\\n                  csrw 0x341, x18\\n\\ncustom_csr_setup: \\n                  nop\\n\\ninit_machine_mode:\\n                  li x18, 0x1800\\n                  csrw 0x300, x18 # MSTATUS\\n                  li x18, 0x0\\n                  csrw 0x'\n",
      "b'304, x18 # MIE\\n                  mret\\ninit:             \\n                  li x0, 0xf1b0ea1f\\n                  li x1, 0xfea23c14\\n                  li x2, 0xbac28bf8\\n                  li x3, 0xe\\n                  li x4, 0x1624842d\\n                  li x5, 0x80000000\\n                  li x6, 0x80000000\\n                  li x7, 0xf7caf781\\n                  li x8, 0xff57ca9a\\n                  li x9, 0x'\n",
      "b'f230bd05\\n                  li x10, 0x80000000\\n                  li x11, 0x4f8b5c46\\n                  li x12, 0x0\\n                  li x14, 0x80000000\\n                  li x15, 0xe\\n                  li x16, 0xf6844dab\\n                  li x17, 0xf3c0dd95\\n                  li x18, 0x80000000\\n                  li x19, 0x80000000\\n                  li x21, 0xf5c0f8aa\\n                  li x22, 0xb97096d5'\n",
      "b'\\n                  li x23, 0x80000000\\n                  li x24, 0xf8dce091\\n                  li x25, 0xd5fcd4c6\\n                  li x26, 0xe\\n                  li x27, 0xe5aebf74\\n                  li x28, 0x80000000\\n                  li x29, 0xce2a50aa\\n                  li x30, 0xd\\n                  li x31, 0xcf21efa9\\n                  la x20, user_stack_end\\nmain:             li           ra, 0x0 #'\n"
     ]
    }
   ],
   "source": [
    "for seq in sequences.take(5):\n",
    "    print(text_from_ids(seq).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "13adfedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(sequence):\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    return input_text, target_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "56dc8b1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
       " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_input_target(list(\"Tensorflow\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a1a549f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d24bc7eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : b'.include \"user_define.h\"\\n.globl _start\\n.section .text\\n_start:           \\n                  .include \"user_init.s\"\\n                  csrr x5, 0xf14\\n                  li x6, 0\\n                  beq x5, x6, 0f\\n\\n0: la x29, h0_start\\njalr x0, x29, 0\\nh0_start:\\n                  li x18, 0x40001104\\n                  csrw 0x301, x18\\nkernel_sp:        \\n                  la x13, kernel_stack_end\\n\\ntrap_vec_ini'\n",
      "Target: b'include \"user_define.h\"\\n.globl _start\\n.section .text\\n_start:           \\n                  .include \"user_init.s\"\\n                  csrr x5, 0xf14\\n                  li x6, 0\\n                  beq x5, x6, 0f\\n\\n0: la x29, h0_start\\njalr x0, x29, 0\\nh0_start:\\n                  li x18, 0x40001104\\n                  csrw 0x301, x18\\nkernel_sp:        \\n                  la x13, kernel_stack_end\\n\\ntrap_vec_init'\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in dataset.take(1):\n",
    "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
    "    print(\"Target:\", text_from_ids(target_example).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4a48698b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=(TensorSpec(shape=(64, 400), dtype=tf.int64, name=None), TensorSpec(shape=(64, 400), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = (\n",
    "    dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3d4f67c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in StringLookup Layer\n",
    "vocab_size = len(ids_from_chars.get_vocabulary())\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7c75a01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "    super().__init__(self)\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True)\n",
    "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "  def call(self, inputs, states=None, return_state=False, training=False):\n",
    "    x = inputs\n",
    "    x = self.embedding(x, training=training)\n",
    "    if states is None:\n",
    "      states = self.gru.get_initial_state(x)\n",
    "    x, states = self.gru(x, initial_state=states, training=training)\n",
    "    x = self.dense(x, training=training)\n",
    "\n",
    "    if return_state:\n",
    "      return x, states\n",
    "    else:\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cc287823",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ef2c47c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 400, 68) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0362bad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       multiple                  17408     \n",
      "                                                                 \n",
      " gru (GRU)                   multiple                  3938304   \n",
      "                                                                 \n",
      " dense (Dense)               multiple                  69700     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,025,412\n",
      "Trainable params: 4,025,412\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "60c50e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c1ef5a4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5, 45, 64, 55, 10, 40, 52, 18, 46, 41, 57, 31, 24, 45, 10, 33, 51,\n",
       "       33, 13, 35, 65, 25, 58, 20, 35, 20, 29, 27, 40, 26,  7, 51,  3, 29,\n",
       "       52, 65, 56, 39, 55, 10,  5, 44, 53, 28, 65, 57, 39, 53, 61, 34, 26,\n",
       "        0, 20, 22, 32, 18, 15, 52, 52, 35, 62, 47, 41, 56, 29, 59, 23,  6,\n",
       "       32, 51, 35, 57, 23, 11, 10, 26,  8, 32, 31, 52, 50,  6, 50, 14, 59,\n",
       "       41, 33,  4,  3, 28,  2, 39, 28,  4, 41, 24, 32, 25,  3, 66, 16, 47,\n",
       "       53, 12, 26,  0, 55, 20, 19, 62, 38, 16, 18, 62, 37, 22,  5, 21, 24,\n",
       "       18,  8, 60, 51, 55, 62, 28, 55, 19, 26, 51, 21, 46, 15, 18, 15, 23,\n",
       "       37, 43, 49, 46, 19, 39, 19,  9,  6, 31, 37,  5, 58, 34, 15, 35,  0,\n",
       "       32, 31, 50, 64,  7,  4, 59, 11, 53,  9, 11, 15,  7, 49, 38, 66, 63,\n",
       "       11, 54, 67, 29, 54, 45, 58,  0, 12, 19, 51, 64, 37, 37,  3, 52,  2,\n",
       "       14, 67, 25,  6, 23, 44, 11, 37, 58, 45, 41, 42, 41, 60, 52, 44, 21,\n",
       "        2, 37, 53, 49, 67,  4, 57, 48, 35, 13, 15, 44,  1, 60, 58, 38, 30,\n",
       "       47, 22, 19, 25, 56, 62,  9, 28, 51, 57, 53, 19, 28, 43, 61, 23, 59,\n",
       "       31, 65, 59, 60, 13, 41, 34, 67, 21, 43,  3, 30, 24, 53,  2, 12, 10,\n",
       "       48,  2, 14, 39, 13, 36, 32, 39, 40, 37,  8, 22, 46, 38, 56, 21, 24,\n",
       "       38, 44, 25, 21, 37, 30, 31, 12, 22, 60, 33, 66, 32, 28, 37, 65, 64,\n",
       "       24, 28,  7, 33, 17, 64, 24,  4, 42, 42, 37, 40, 36, 40, 52, 60, 40,\n",
       "       37, 34, 27, 66, 44,  3, 23,  5, 19, 65,  9, 47, 42, 26, 25, 31, 30,\n",
       "       45, 58, 51, 42, 27, 11, 12, 30, 11,  4, 55, 48, 42, 28, 64, 36,  9,\n",
       "       58, 35, 50, 36, 59,  1, 54, 42, 33, 61, 48, 23, 27, 23, 46,  7, 49,\n",
       "        0, 55, 10, 33,  4,  1, 66,  1, 41, 15, 60, 29, 29, 41, 17, 37, 42,\n",
       "       16, 15, 66, 28, 55,  3, 30, 66,  9,  2, 33, 13,  4, 40, 46, 33, 58,\n",
       "       33, 53, 23, 47,  8, 17, 35, 22, 27], dtype=int64)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "da12f7ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      " b', gp\\n                  mulh         a2, t6, a6\\n                  c.nop\\n                  divu         gp, ra, s5\\n                  sll          t1, t0, t5\\n                  c.xor        a1, a2\\n                  div          a1, gp, s7\\n                  mulhsu       t2, a2, s0\\n                  mulhsu       t1, a1, s7\\n                  c.slli       t3, 30\\n                  c.srli       a1, 25\\n     '\n",
      "\n",
      "Next Char Predictions:\n",
      " b'(dwn0Vk8e_pLBd0NjN3PxCq:P:IEVD,j\"IkxoUn0(clGxpUltOD[UNK]:@M85kkPuf_oIrA)MjPpA10D-MLki)i4r_N#\"G UG#_BMC\"y6fl2D[UNK]n:9uT68uS@(;B8-sjnuGn9Dj;e585ASbhe9U9.)LS(qO5P[UNK]MLiw,#r1l.15,hTyv1mzImdq[UNK]29jwSS\"k 4zC)Ac1Sqd_a_skc; Slhz#pgP35c\\nsqTKf@9Cou.Gjpl9GbtArLxrs3_Oz;b\"KBl 20g 4U3RMUVS-@eTo;BTcC;SKL2@sNyMGSxwBG,N7wB#aaSVRVksVSOEyc\"A(9x.faDCLKdqjaE12K1#ngaGwR.qPiRr\\nmaNtgAEAe,h[UNK]n0N#\\ny\\n_5sII_7Sa65yGn\"Ky. N3#VeNqNlAf-7P@E'\n"
     ]
    }
   ],
   "source": [
    "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
    "print()\n",
    "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "afbec988",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8ae17e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 400, 68)  # (batch_size, sequence_length, vocab_size)\n",
      "Mean loss:         tf.Tensor(4.225536, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"Mean loss:        \", example_batch_mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "82c9d080",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68.411156"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.exp(example_batch_mean_loss).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7a0bcd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "969de61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1b926ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "29f1a911",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "18/18 [==============================] - 198s 11s/step - loss: 3.1897\n",
      "Epoch 2/20\n",
      "18/18 [==============================] - 212s 12s/step - loss: 1.4539\n",
      "Epoch 3/20\n",
      "18/18 [==============================] - 221s 12s/step - loss: 1.2489\n",
      "Epoch 4/20\n",
      "18/18 [==============================] - 214s 12s/step - loss: 1.0908\n",
      "Epoch 5/20\n",
      "18/18 [==============================] - 196s 11s/step - loss: 0.9354\n",
      "Epoch 6/20\n",
      "18/18 [==============================] - 198s 11s/step - loss: 0.8133\n",
      "Epoch 7/20\n",
      "18/18 [==============================] - 214s 12s/step - loss: 0.7195\n",
      "Epoch 8/20\n",
      "18/18 [==============================] - 213s 12s/step - loss: 0.6522\n",
      "Epoch 9/20\n",
      "18/18 [==============================] - 215s 12s/step - loss: 0.5973\n",
      "Epoch 10/20\n",
      "18/18 [==============================] - 203s 11s/step - loss: 0.5500\n",
      "Epoch 11/20\n",
      "18/18 [==============================] - 211s 12s/step - loss: 0.5095\n",
      "Epoch 12/20\n",
      "18/18 [==============================] - 206s 11s/step - loss: 0.4888\n",
      "Epoch 13/20\n",
      "18/18 [==============================] - 212s 12s/step - loss: 0.4732\n",
      "Epoch 14/20\n",
      "18/18 [==============================] - 234s 13s/step - loss: 0.4528\n",
      "Epoch 15/20\n",
      "18/18 [==============================] - 225s 12s/step - loss: 0.4432\n",
      "Epoch 16/20\n",
      "18/18 [==============================] - 219s 12s/step - loss: 0.4219\n",
      "Epoch 17/20\n",
      "18/18 [==============================] - 371s 21s/step - loss: 0.4325\n",
      "Epoch 18/20\n",
      "18/18 [==============================] - 388s 22s/step - loss: 0.4080\n",
      "Epoch 19/20\n",
      "18/18 [==============================] - 189s 10s/step - loss: 0.3894\n",
      "Epoch 20/20\n",
      "18/18 [==============================] - 186s 10s/step - loss: 0.3788\n"
     ]
    }
   ],
   "source": [
    "EPOCHS=20\n",
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b2db81bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneStep(tf.keras.Model):\n",
    "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
    "    super().__init__()\n",
    "    self.temperature = temperature\n",
    "    self.model = model\n",
    "    self.chars_from_ids = chars_from_ids\n",
    "    self.ids_from_chars = ids_from_chars\n",
    "\n",
    "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
    "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
    "    sparse_mask = tf.SparseTensor(\n",
    "        # Put a -inf at each bad index.\n",
    "        values=[-float('inf')]*len(skip_ids),\n",
    "        indices=skip_ids,\n",
    "        # Match the shape to the vocabulary\n",
    "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
    "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "\n",
    "  @tf.function\n",
    "  def generate_one_step(self, inputs, states=None):\n",
    "    # Convert strings to token IDs.\n",
    "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
    "\n",
    "    # Run the model.\n",
    "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
    "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
    "                                          return_state=True)\n",
    "    # Only use the last prediction.\n",
    "    predicted_logits = predicted_logits[:, -1, :]\n",
    "    predicted_logits = predicted_logits/self.temperature\n",
    "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
    "    predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "    # Sample the output logits to generate token IDs.\n",
    "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "    # Convert from token ids to characters\n",
    "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
    "\n",
    "    # Return the characters and model state.\n",
    "    return predicted_chars, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d4fb9579",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5df38a69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_define.h_st_nta, s6f\n",
      "                 rem          s5, 579018\n",
      "                  lw  x17, 00(x25\n",
      "                  lw   x16, 00(x20)\n",
      "                  c.lw 0x3710, 31s\n",
      "                  li x19, x1O, 0\n",
      "                  li           55, 0x3\n",
      "                  srai x18, x10, x80\n",
      "                  rem x22, 0\n",
      "                  lemu          t6, 0x3605000\n",
      "                  li           st, 0x00b                  re, x28, 800\n",
      "                  lw  x10, 60(x20)\n",
      "                  lw  x26, 700(x28)\n",
      "                  sw x10, 114(x20)\n",
      "                  rem x29, 2\n",
      "                   srli x26, 0x80026a3, a1\n",
      "                  jslliux x2, a1, 29\n",
      "                  slli         s11, t1, zero\n",
      "                  slt          t0, ra, s20\n",
      "                   mulh         t0, s7, a4\n",
      "                  addi         s0, a4, 998\n",
      "                  c.li         s0, 23\n",
      "                  addi         s8, a1, 4779\n",
      "                  c.add        t3, s1\n",
      "                  c.sub        s5, a1\n",
      "                  c.xor        a5,#s9\n",
      "  \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 2.3964996337890625\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(['user_define.h'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "    next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "    result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
    "print('\\nRun time:', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0828831e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernel_sp: 22\n",
      "                 div          s1, s9, t1\n",
      "                   c.li         s9, 21\n",
      "                  c.addi16sp   sp, -16\n",
      "                  slt          s7, s0, a5\n",
      "                  srli         s9, s2, s9\n",
      "                  c.srli       a0, 8\n",
      "                  add          a0, a6, t4\n",
      "                  c.or         tp, a3\n",
      "                  remu         t2, s6, tp\n",
      "                  slli         s0, s2, -894\n",
      "                  mulh          a4, s10, s1\n",
      "                  srli         t1, gp, -1180\n",
      "                  c.srai       s1, 2\n",
      "                  c.sp, sp, -3\n",
      "                  c.sub        s7, a0\n",
      "                  c.slli       a5, 13\n",
      "                  mulh         t1, a1, s4\n",
      "                  nop\n",
      "                  c.li         s1, -1\n",
      "                  or           t6, gp, s4\n",
      "                   c.slli       a0, 32\n",
      "                  c.or         s8, a5\n",
      "                  divu         s10, gp, s6\n",
      "                  c.srli       a1, 18\n",
      "                  c.lui        a1, 37\n",
      "                  \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 1.4040396213531494\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(['kernel_sp:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "    next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "    result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
    "print('\\nRun time:', end - start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
